{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a131812e-af9d-48d0-9859-01a4921c7826",
   "metadata": {},
   "source": [
    "# Training and packaging Spacy\n",
    "\n",
    "- In this notebook, we assume that we already have a usable vector space, contained in the `embeddings.txt` file. To see how we compile such a vector space, see the notebook `02-fastext-vectors.ipynb`\n",
    "\n",
    "## Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26c357-cac3-438e-b3b1-9305bbe0210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3822d2-b464-4f6f-afc5-bc701a57f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path_or_uri: str, exist_ok: bool = True):\n",
    "    return os.makedirs(path_or_uri, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cc622-3b91-42e6-87b1-05e06ae53242",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090b6c1-589f-413e-af97-8c1c645a8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'nb'\n",
    "n_dims = 300               # vector dimensions\n",
    "n_vectors = 5000           # in production we use 50k\n",
    "vector_arch = 'fasttext'   # just used in model name\n",
    "version = '2.0.0'\n",
    "batch_size = 2000\n",
    "patience = 1000            # 10000 better for real modelling\n",
    "                           # very important parameter: how long SpaCy will run the training process before early stopping\n",
    "                           # we do not know how long it will really take...\n",
    "\n",
    "pkg_name_short = f'nhst_{vector_arch}_{n_dims}_{n_vectors}'\n",
    "pkg_name_full = f'{lang}_{pkg_name_short}-{version}'\n",
    "\n",
    "vectors_dir = f'nhst_{n_dims}'\n",
    "job_dir = f'job_data'\n",
    "\n",
    "# ready to use GPU: install pytorch in environment\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_id = 0\n",
    "    gpu_allocator = '\"pytorch\"'  # double quotes needed\n",
    "    spacy.require_gpu()\n",
    "else:\n",
    "    gpu_id = -1\n",
    "    gpu_allocator = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca95184-d451-4c3e-9427-5e8aad233ebf",
   "metadata": {},
   "source": [
    "## Init vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe71e7-fd66-4a9a-bb7c-6e6a1bb4ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have a simple vector file... this should be computed in advance!\n",
    "embeddings_local = 'embeddings.txt'\n",
    "\n",
    "make_dirs(vectors_dir)\n",
    "\n",
    "# run spacy init vectors\n",
    "args = ['python', '-m', 'spacy', 'init', 'vectors', lang, embeddings_local, vectors_dir, '--prune', str(n_vectors),\n",
    "        '--name', f'nhst_{n_dims}']\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ea275-8ec9-486c-ad97-f9f96a47f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init vectors nb embeddings.txt nhst_300 --prune 10000 --name nhst_300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08898236-60b6-4343-af3c-36cdec24a1e5",
   "metadata": {},
   "source": [
    "## Write job-setup files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7779749-062d-459c-9720-a40c64ab3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "yml_config_str = '''\n",
    "title: \"Norwegian POS Tagging, Dependency Parsing (Universal Dependencies) and NER with Norne\"\n",
    "description: \"Template to train a POS tagger, morphologizer, dependency parser amd named entity recogniser from a\n",
    "[Universal Dependencies](https://universaldependencies.org/) corpus, in its Norne version.\n",
    "It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model.\"\n",
    "\n",
    "vars:\n",
    "  config: \"default\"\n",
    "  lang: \"{package_lang}\"\n",
    "  treebank: \"norne\"\n",
    "  train_name: \"no_bokmaal-ud-train\"\n",
    "  dev_name: \"no_bokmaal-ud-dev\"\n",
    "  test_name: \"no_bokmaal-ud-test\"\n",
    "  package_name: \"{package_name}\"\n",
    "  package_version: \"{package_version}\"\n",
    "  gpu: {gpu}\n",
    "\n",
    "# These are the directories that the project needs. The project CLI will make sure that they always exist.\n",
    "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
    "\n",
    "assets:\n",
    "  - dest: \"assets/${{vars.treebank}}\"\n",
    "    git:\n",
    "      repo: \"https://github.com/ltgoslo/${{vars.treebank}}\"  # \"https://github.com/UniversalDependencies/${{vars.treebank}}\"\n",
    "      branch: \"master\"\n",
    "      path: \"\"\n",
    "\n",
    "workflows:\n",
    "  all:\n",
    "    - preprocess\n",
    "    - train\n",
    "    - evaluate\n",
    "    - package\n",
    "\n",
    "commands:\n",
    "  - name: preprocess\n",
    "    help: \"Convert the data to spaCy's format\"\n",
    "    script:\n",
    "      - \"mkdir -p corpus/${{vars.treebank}}\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.train_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.dev_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.test_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.train_name}}.spacy corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.dev_name}}.spacy corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.test_name}}.spacy corpus/${{vars.treebank}}/test.spacy\"\n",
    "    deps:\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.train_name}}.conllu\"\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.dev_name}}.conllu\"\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.test_name}}.conllu\"\n",
    "    outputs:\n",
    "      - \"corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/test.spacy\"\n",
    "\n",
    "  - name: train\n",
    "    help: \"Train ${{vars.treebank}}\"\n",
    "    script:\n",
    "      - \"python -m spacy train configs/${{vars.config}}.cfg --output training/${{vars.treebank}} --gpu-id ${{vars.gpu}} --paths.train corpus/${{vars.treebank}}/train.spacy --paths.dev corpus/${{vars.treebank}}/dev.spacy --nlp.lang=${{vars.lang}}\"\n",
    "    deps:\n",
    "      - \"corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"configs/${{vars.config}}.cfg\"\n",
    "    outputs:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "\n",
    "  - name: evaluate\n",
    "    help: \"Evaluate on the test data and save the metrics\"\n",
    "    script:\n",
    "      - \"python -m spacy evaluate ./training/${{vars.treebank}}/model-best ./corpus/${{vars.treebank}}/test.spacy --output ./metrics/metrics.json --gpu-id ${{vars.gpu}}\"\n",
    "    deps:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "      - \"corpus/${{vars.treebank}}/test.spacy\"\n",
    "    outputs:\n",
    "      - \"metrics/metrics.json\"\n",
    "\n",
    "  - name: package\n",
    "    help: \"Package the trained model so it can be installed\"\n",
    "    script:\n",
    "      - \"python -m spacy package training/${{vars.treebank}}/model-best packages --name ${{vars.package_name}} --version ${{vars.package_version}} --force\"\n",
    "    deps:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "    outputs_no_cache:\n",
    "      - \"packages/${{vars.lang}}_${{vars.package_name}}-${{vars.package_version}}/dist/${{vars.lang}}_${{vars.package_name}}-${{vars.package_version}}.tar.gz\"\n",
    "\n",
    "  - name: clean\n",
    "    help: \"Remove intermediate files\"\n",
    "    script:\n",
    "      - \"rm -rf training/*\"\n",
    "      - \"rm -rf metrics/*\"\n",
    "      - \"rm -rf corpus/*\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c612f-c4eb-4656-bc2a-ad55c7a4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Norwegian official package (large), plus GPU\n",
    "exp_config_str = '''\n",
    "[paths]\n",
    "train = \"corpus/norne/train.spacy\"\n",
    "dev = \"corpus/norne/dev.spacy\"\n",
    "vectors = \"../{vectors_dir}\"\n",
    "init_tok2vec = null\n",
    "\n",
    "[system]\n",
    "gpu_allocator = {allocator}\n",
    "seed = 0\n",
    "\n",
    "[nlp]\n",
    "lang = \"nb\"\n",
    "pipeline = [\"tok2vec\",\"morphologizer\",\"parser\",\"lemmatizer\",\"senter\",\"attribute_ruler\",\"ner\"]\n",
    "disabled = [\"senter\"]\n",
    "before_creation = null\n",
    "after_creation = null\n",
    "after_pipeline_creation = null\n",
    "batch_size = {batch_size}\n",
    "tokenizer = {{\"@tokenizers\":\"spacy.Tokenizer.v1\"}}\n",
    "vectors = {{\"@vectors\":\"spacy.Vectors.v1\"}}\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.attribute_ruler]\n",
    "factory = \"attribute_ruler\"\n",
    "scorer = {{\"@scorers\":\"spacy.attribute_ruler_scorer.v1\"}}\n",
    "validate = false\n",
    "\n",
    "[components.lemmatizer]\n",
    "factory = \"trainable_lemmatizer\"\n",
    "backoff = \"orth\"\n",
    "min_tree_freq = 3\n",
    "overwrite = false\n",
    "scorer = {{\"@scorers\":\"spacy.lemmatizer_scorer.v1\"}}\n",
    "top_k = 1\n",
    "\n",
    "[components.lemmatizer.model]\n",
    "@architectures = \"spacy.Tagger.v2\"\n",
    "nO = null\n",
    "normalize = false\n",
    "\n",
    "[components.lemmatizer.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "upstream = \"tok2vec\"\n",
    "\n",
    "[components.morphologizer]\n",
    "factory = \"morphologizer\"\n",
    "extend = false\n",
    "label_smoothing = 0.0\n",
    "overwrite = true\n",
    "scorer = {{\"@scorers\":\"spacy.morphologizer_scorer.v1\"}}\n",
    "\n",
    "[components.morphologizer.model]\n",
    "@architectures = \"spacy.Tagger.v2\"\n",
    "nO = null\n",
    "normalize = false\n",
    "\n",
    "[components.morphologizer.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "upstream = \"tok2vec\"\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "incorrect_spans_key = null\n",
    "moves = null\n",
    "scorer = {{\"@scorers\":\"spacy.ner_scorer.v1\"}}\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.ner.model.tok2vec.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = 96\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [5000,1000,2500,2500]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.ner.model.tok2vec.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.parser]\n",
    "factory = \"parser\"\n",
    "learn_tokens = false\n",
    "min_action_freq = 30\n",
    "moves = null\n",
    "scorer = {{\"@scorers\":\"spacy.parser_scorer.v1\"}}\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.parser.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"parser\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.parser.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "upstream = \"tok2vec\"\n",
    "\n",
    "[components.senter]\n",
    "factory = \"senter\"\n",
    "overwrite = false\n",
    "scorer = {{\"@scorers\":\"spacy.senter_scorer.v1\"}}\n",
    "\n",
    "[components.senter.model]\n",
    "@architectures = \"spacy.Tagger.v2\"\n",
    "nO = null\n",
    "normalize = false\n",
    "\n",
    "[components.senter.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.senter.model.tok2vec.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = 16\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\",\"SPACY\"]\n",
    "rows = [1000,500,500,500,50]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.senter.model.tok2vec.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 16\n",
    "depth = 2\n",
    "window_size = 1\n",
    "maxout_pieces = 2\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\",\"SPACY\",\"IS_SPACE\"]\n",
    "rows = [5000,1000,2500,2500,50,50]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${{paths.dev}}\n",
    "gold_preproc = false\n",
    "max_length = 0\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${{paths.train}}\n",
    "gold_preproc = false\n",
    "max_length = 0\n",
    "limit = 0\n",
    "augmenter = null\n",
    "\n",
    "[training]\n",
    "train_corpus = \"corpora.train\"\n",
    "dev_corpus = \"corpora.dev\"\n",
    "seed = ${{system:seed}}\n",
    "gpu_allocator = ${{system:gpu_allocator}}\n",
    "dropout = 0.1\n",
    "accumulate_gradient = 1\n",
    "patience = {patience}\n",
    "max_epochs = 0\n",
    "max_steps = 100000\n",
    "eval_frequency = 1000\n",
    "frozen_components = []\n",
    "before_to_disk = null\n",
    "annotating_components = []\n",
    "before_update = null\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "get_length = null\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "t = 0.0\n",
    "\n",
    "[training.logger]\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "progress_bar = false\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "L2_is_weight_decay = true\n",
    "L2 = 0.01\n",
    "grad_clip = 1.0\n",
    "use_averages = true\n",
    "eps = 0.00000001\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training.score_weights]\n",
    "pos_acc = 0.14\n",
    "morph_acc = 0.14\n",
    "morph_per_feat = null\n",
    "dep_uas = 0.0\n",
    "dep_las = 0.29\n",
    "dep_las_per_type = null\n",
    "sents_p = null\n",
    "sents_r = null\n",
    "sents_f = 0.04\n",
    "lemma_acc = 0.1\n",
    "ents_f = 0.29\n",
    "ents_p = 0.0\n",
    "ents_r = 0.0\n",
    "ents_per_type = null\n",
    "speed = 0.0\n",
    "\n",
    "[pretraining]\n",
    "\n",
    "[initialize]\n",
    "vocab_data = null\n",
    "vectors = ${{paths.vectors}}\n",
    "init_tok2vec = ${{paths.init_tok2vec}}\n",
    "before_init = null\n",
    "after_init = null\n",
    "\n",
    "[initialize.components]\n",
    "\n",
    "[initialize.components.lemmatizer]\n",
    "\n",
    "[initialize.components.lemmatizer.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/trainable_lemmatizer.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.components.morphologizer]\n",
    "\n",
    "[initialize.components.morphologizer.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/morphologizer.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.components.ner]\n",
    "\n",
    "[initialize.components.ner.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/ner.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.components.parser]\n",
    "\n",
    "[initialize.components.parser.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/parser.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.lookups]\n",
    "@misc = \"spacy.LookupsDataLoader.v1\"\n",
    "lang = ${{nlp.lang}}\n",
    "tables = []\n",
    "\n",
    "[initialize.tokenizer]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c945a7c-8abd-4896-8856-8d02e2151fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save configuration strings to files\n",
    "make_dirs(job_dir)\n",
    "with open(f'{job_dir}/project.yml', 'w') as f:\n",
    "    f.write(yml_config_str.format(package_lang=lang,\n",
    "                                  package_name=pkg_name_short,\n",
    "                                  package_version=version,\n",
    "                                  gpu=gpu_id))\n",
    "\n",
    "make_dirs(f'{job_dir}/configs')\n",
    "with open(f'{job_dir}/configs/default.cfg', 'w') as f:\n",
    "    f.write(exp_config_str.format(vectors_dir=vectors_dir,\n",
    "                                  allocator=gpu_allocator,\n",
    "                                  batch_size=batch_size,\n",
    "                                  patience=patience))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d92ca-840e-490e-bf71-59ebd139b9f4",
   "metadata": {},
   "source": [
    "## Downloading data assets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded8afe-4e01-4d18-95f2-37f462a4f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['python', '-m', 'spacy', 'project', 'assets', job_dir]\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efff419-c104-4fdb-8acb-778f64c16381",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy project assets job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7694350-daa4-4a0c-bb7e-d9c540f86a6c",
   "metadata": {},
   "source": [
    "## Running job tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01c46b-feed-46ba-9ed3-53c24a088e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['python', '-m', 'spacy', 'project', 'run', 'all', job_dir]\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65c0d2-82d5-41eb-b496-c244a29a9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy project run all job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3e69e-60fe-4361-bb64-4c9233e23e96",
   "metadata": {},
   "source": [
    "## Saving output\n",
    "\n",
    "- the necessary files are already saved, but we should move them to the final location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69805fbf-52ea-4c03-9701-4ce62daf9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built automatically by Spacy, but we need it to copy the package\n",
    "pkg_file = f'{job_dir}/packages/{pkg_name_full}/dist/{pkg_name_full}.tar.gz'\n",
    "print(pkg_file)\n",
    "\n",
    "# evaluation metrics\n",
    "metrics_file = f'{job_dir}/metrics/metrics.json'\n",
    "print(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c806b-f6b5-4374-8765-084987e2cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metrics\n",
    "with open(f'{job_dir}/metrics/metrics.json') as f:\n",
    "    metrics = json.load(f)\n",
    "    for k, v in metrics.items():\n",
    "        print('##', k, '##')\n",
    "        print(v)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf6092-d89a-4857-b508-44a214917365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfedc12-1956-4c01-88fa-19a08c94c487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
