{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a131812e-af9d-48d0-9859-01a4921c7826",
   "metadata": {},
   "source": [
    "# Training and packaging Spacy\n",
    "\n",
    "- In this notebook, we assume that we already have a usable vector space, contained in the `embeddings.txt` file. To see how we compile such a vector space, see the notebook `02-fastext-vectors.ipynb`\n",
    "\n",
    "## Imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c26c357-cac3-438e-b3b1-9305bbe0210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3822d2-b464-4f6f-afc5-bc701a57f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path_or_uri: str, exist_ok: bool = True):\n",
    "    return os.makedirs(path_or_uri, exist_ok=exist_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7cc622-3b91-42e6-87b1-05e06ae53242",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3090b6c1-589f-413e-af97-8c1c645a8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'nb'\n",
    "n_dims = 300               # vector dimensions\n",
    "n_vectors = 10000          # in production we use 50k\n",
    "vector_arch = 'fasttext'   # just used in model name\n",
    "version = '2.0.0'\n",
    "batch_size = 2000\n",
    "patience = 1200            # 10000 better for real modelling\n",
    "                           # very important parameter: how long SpaCy will run the training process before early stopping\n",
    "                           # we do not know how long it will really take...\n",
    "\n",
    "pkg_name_short = f'nhst_{vector_arch}_{n_dims}_{n_vectors}'\n",
    "pkg_name_full = f'{lang}_{pkg_name_short}-{version}'\n",
    "\n",
    "vectors_dir = f'nhst_{n_dims}'\n",
    "job_dir = f'job_data'\n",
    "\n",
    "# ready to use GPU: install pytorch in environment\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    gpu_id = 0\n",
    "    gpu_allocator = '\"pytorch\"'  # double quotes needed\n",
    "    spacy.require_gpu()\n",
    "else:\n",
    "    gpu_id = -1\n",
    "    gpu_allocator = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca95184-d451-4c3e-9427-5e8aad233ebf",
   "metadata": {},
   "source": [
    "## Init vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafe71e7-fd66-4a9a-bb7c-6e6a1bb4ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m spacy init vectors nb embeddings.txt nhst_300 --prune 10000 --name nhst_300\n"
     ]
    }
   ],
   "source": [
    "# we already have a simple vector file... this should be computed in advance!\n",
    "embeddings_local = 'embeddings.txt'\n",
    "\n",
    "make_dirs(vectors_dir)\n",
    "\n",
    "# run spacy init vectors\n",
    "args = ['python', '-m', 'spacy', 'init', 'vectors', lang, embeddings_local, vectors_dir, '--prune', str(n_vectors),\n",
    "        '--name', f'nhst_{n_dims}']\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129ea275-8ec9-486c-ad97-f9f96a47f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Creating blank nlp object for language 'nb'\u001B[0m\n",
      "20000it [00:00, 22055.90it/s]\n",
      "\u001B[38;5;2m✔ Successfully converted 5000 vectors\u001B[0m\n",
      "\u001B[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
      "the path to it in your config as the 'vectors' setting in [initialize].\u001B[0m\n",
      "/Users/emiliano/Development/spacy_norwegian_training_test/nhst_300\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init vectors nb embeddings.txt nhst_300 --prune 5000 --name nhst_300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08898236-60b6-4343-af3c-36cdec24a1e5",
   "metadata": {},
   "source": [
    "## Write job-setup files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7779749-062d-459c-9720-a40c64ab3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "yml_config_str = '''\n",
    "title: \"Norwegian POS Tagging, Dependency Parsing (Universal Dependencies) and NER with Norne\"\n",
    "description: \"Template to train a POS tagger, morphologizer, dependency parser amd named entity recogniser from a\n",
    "[Universal Dependencies](https://universaldependencies.org/) corpus, in its Norne version.\n",
    "It takes care of downloading the treebank, converting it to spaCy's format and training and evaluating the model.\"\n",
    "\n",
    "vars:\n",
    "  config: \"default\"\n",
    "  lang: \"{package_lang}\"\n",
    "  treebank: \"norne\"\n",
    "  train_name: \"no_bokmaal-ud-train\"\n",
    "  dev_name: \"no_bokmaal-ud-dev\"\n",
    "  test_name: \"no_bokmaal-ud-test\"\n",
    "  package_name: \"{package_name}\"\n",
    "  package_version: \"{package_version}\"\n",
    "  gpu: {gpu}\n",
    "\n",
    "# These are the directories that the project needs. The project CLI will make sure that they always exist.\n",
    "directories: [\"assets\", \"corpus\", \"training\", \"metrics\", \"configs\", \"packages\"]\n",
    "\n",
    "assets:\n",
    "  - dest: \"assets/${{vars.treebank}}\"\n",
    "    git:\n",
    "      repo: \"https://github.com/ltgoslo/${{vars.treebank}}\"  # \"https://github.com/UniversalDependencies/${{vars.treebank}}\"\n",
    "      branch: \"master\"\n",
    "      path: \"\"\n",
    "\n",
    "workflows:\n",
    "  all:\n",
    "    - preprocess\n",
    "    - train\n",
    "    - evaluate\n",
    "    - package\n",
    "\n",
    "commands:\n",
    "  - name: preprocess\n",
    "    help: \"Convert the data to spaCy's format\"\n",
    "    script:\n",
    "      - \"mkdir -p corpus/${{vars.treebank}}\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.train_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.dev_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"python -m spacy convert assets/${{vars.treebank}}/ud/nob/${{vars.test_name}}.conllu corpus/${{vars.treebank}}/ --converter conllu --n-sents 10 --merge-subtokens\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.train_name}}.spacy corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.dev_name}}.spacy corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"mv corpus/${{vars.treebank}}/${{vars.test_name}}.spacy corpus/${{vars.treebank}}/test.spacy\"\n",
    "    deps:\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.train_name}}.conllu\"\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.dev_name}}.conllu\"\n",
    "      - \"assets/${{vars.treebank}}/ud/nob/${{vars.test_name}}.conllu\"\n",
    "    outputs:\n",
    "      - \"corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/test.spacy\"\n",
    "\n",
    "  - name: train\n",
    "    help: \"Train ${{vars.treebank}}\"\n",
    "    script:\n",
    "      - \"python -m spacy train configs/${{vars.config}}.cfg --output training/${{vars.treebank}} --gpu-id ${{vars.gpu}} --paths.train corpus/${{vars.treebank}}/train.spacy --paths.dev corpus/${{vars.treebank}}/dev.spacy --nlp.lang=${{vars.lang}}\"\n",
    "    deps:\n",
    "      - \"corpus/${{vars.treebank}}/train.spacy\"\n",
    "      - \"corpus/${{vars.treebank}}/dev.spacy\"\n",
    "      - \"configs/${{vars.config}}.cfg\"\n",
    "    outputs:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "\n",
    "  - name: evaluate\n",
    "    help: \"Evaluate on the test data and save the metrics\"\n",
    "    script:\n",
    "      - \"python -m spacy evaluate ./training/${{vars.treebank}}/model-best ./corpus/${{vars.treebank}}/test.spacy --output ./metrics/metrics.json --gpu-id ${{vars.gpu}}\"\n",
    "    deps:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "      - \"corpus/${{vars.treebank}}/test.spacy\"\n",
    "    outputs:\n",
    "      - \"metrics/metrics.json\"\n",
    "\n",
    "  - name: package\n",
    "    help: \"Package the trained model so it can be installed\"\n",
    "    script:\n",
    "      - \"python -m spacy package training/${{vars.treebank}}/model-best packages --name ${{vars.package_name}} --version ${{vars.package_version}} --force\"\n",
    "    deps:\n",
    "      - \"training/${{vars.treebank}}/model-best\"\n",
    "    outputs_no_cache:\n",
    "      - \"packages/${{vars.lang}}_${{vars.package_name}}-${{vars.package_version}}/dist/${{vars.lang}}_${{vars.package_name}}-${{vars.package_version}}.tar.gz\"\n",
    "\n",
    "  - name: clean\n",
    "    help: \"Remove intermediate files\"\n",
    "    script:\n",
    "      - \"rm -rf training/*\"\n",
    "      - \"rm -rf metrics/*\"\n",
    "      - \"rm -rf corpus/*\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82c612f-c4eb-4656-bc2a-ad55c7a4e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on English and Norwegian official packages, plus GPU, plus static vectors and training tweaks\n",
    "# 512 to 10k batch size, 0.15 dropout, 10k patience, larger matrix sizes, 6k tok2vec layer, eval 1k\n",
    "exp_config_str = '''\n",
    "[paths]\n",
    "train = \"corpus/norne/train.spacy\"\n",
    "dev = \"corpus/norne/dev.spacy\"\n",
    "vectors = \"../{vectors_dir}\"\n",
    "raw = null\n",
    "init_tok2vec = null\n",
    "vocab_data = null\n",
    "\n",
    "[system]\n",
    "gpu_allocator = {allocator}\n",
    "seed = 0\n",
    "\n",
    "[nlp]\n",
    "lang = \"nb\"\n",
    "pipeline = [\"tok2vec\",\"morphologizer\",\"parser\",\"senter\",\"attribute_ruler\",\"lemmatizer\",\"ner\"]\n",
    "disabled = [\"senter\"]\n",
    "before_creation = null\n",
    "after_creation = null\n",
    "after_pipeline_creation = null\n",
    "batch_size = {batch_size}\n",
    "tokenizer = {{\"@tokenizers\":\"spacy.Tokenizer.v1\"}}\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [6000,3000,3000,3000]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.parser]\n",
    "factory = \"parser\"\n",
    "learn_tokens = false\n",
    "min_action_freq = 30\n",
    "moves = null\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.parser.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"parser\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.parser.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "upstream = \"tok2vec\"\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "moves = null\n",
    "update_with_oracle_cut_size = 100\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "state_type = \"ner\"\n",
    "extra_state_tokens = false\n",
    "hidden_width = 64\n",
    "maxout_pieces = 2\n",
    "use_upper = true\n",
    "nO = null\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.ner.model.tok2vec.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = 96\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [6000,3000,3000,3000]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.ner.model.tok2vec.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.morphologizer]\n",
    "factory = \"morphologizer\"\n",
    "\n",
    "[components.morphologizer.model]\n",
    "@architectures = \"spacy.Tagger.v1\"\n",
    "nO = null\n",
    "\n",
    "[components.morphologizer.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${{components.tok2vec.model.encode:width}}\n",
    "upstream = \"tok2vec\"\n",
    "\n",
    "[components.attribute_ruler]\n",
    "factory = \"attribute_ruler\"\n",
    "validate = false\n",
    "\n",
    "[components.lemmatizer]\n",
    "factory = \"lemmatizer\"\n",
    "mode = \"rule\"\n",
    "model = null\n",
    "overwrite = false\n",
    "\n",
    "[components.senter]\n",
    "factory = \"senter\"\n",
    "\n",
    "[components.senter.model]\n",
    "@architectures = \"spacy.Tagger.v1\"\n",
    "nO = null\n",
    "\n",
    "[components.senter.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.senter.model.tok2vec.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = 16\n",
    "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
    "rows = [1200,600,600,600]\n",
    "include_static_vectors = true\n",
    "\n",
    "[components.senter.model.tok2vec.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 16\n",
    "depth = 2\n",
    "window_size = 1\n",
    "maxout_pieces = 2\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${{paths:train}}\n",
    "max_length = 5000\n",
    "gold_preproc = false\n",
    "limit = 0\n",
    "\n",
    "[corpora.train.augmenter]\n",
    "@augmenters = \"spacy.lower_case.v1\"\n",
    "level = 0.1\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "limit = 0\n",
    "max_length = 0\n",
    "path = ${{paths:dev}}\n",
    "gold_preproc = false\n",
    "augmenter = null\n",
    "\n",
    "[training]\n",
    "train_corpus = \"corpora.train\"\n",
    "dev_corpus = \"corpora.dev\"\n",
    "seed = ${{system:seed}}\n",
    "gpu_allocator = ${{system:gpu_allocator}}\n",
    "dropout = 0.15\n",
    "accumulate_gradient = 1\n",
    "patience = {patience}\n",
    "max_epochs = 0\n",
    "max_steps = 0\n",
    "eval_frequency = 1000\n",
    "frozen_components = []\n",
    "before_to_disk = null\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "get_length = null\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "t = 0.0\n",
    "\n",
    "[training.logger]\n",
    "@loggers = \"spacy.ConsoleLogger.v1\"\n",
    "progress_bar = false\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "L2_is_weight_decay = true\n",
    "L2 = 0.01\n",
    "grad_clip = 1.0\n",
    "use_averages = true\n",
    "eps = 0.00000001\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training.score_weights]\n",
    "pos_acc = 0.16\n",
    "morph_acc = 0.08\n",
    "morph_per_feat = null\n",
    "dep_uas = 0.0\n",
    "dep_las = 0.18\n",
    "dep_las_per_type = null\n",
    "sents_p = null\n",
    "sents_r = null\n",
    "sents_f = 0.04\n",
    "lemma_acc = 0.14\n",
    "ents_f = 0.3\n",
    "ents_p = 0.1\n",
    "ents_r = 0.0\n",
    "ents_per_type = null\n",
    "\n",
    "[pretraining]\n",
    "\n",
    "[initialize]\n",
    "vocab_data = ${{paths.vocab_data}}\n",
    "vectors = ${{paths.vectors}}\n",
    "init_tok2vec = ${{paths.init_tok2vec}}\n",
    "before_init = null\n",
    "after_init = null\n",
    "\n",
    "[initialize.components]\n",
    "\n",
    "[initialize.components.morphologizer]\n",
    "\n",
    "[initialize.components.morphologizer.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/morphologizer.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.components.ner]\n",
    "\n",
    "[initialize.components.ner.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/ner.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.components.parser]\n",
    "\n",
    "[initialize.components.parser.labels]\n",
    "@readers = \"spacy.read_labels.v1\"\n",
    "path = \"corpus/labels/parser.json\"\n",
    "require = false\n",
    "\n",
    "[initialize.lookups]\n",
    "@misc = \"spacy.LookupsDataLoader.v1\"\n",
    "lang = ${{nlp.lang}}\n",
    "tables = []\n",
    "\n",
    "[initialize.tokenizer]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c945a7c-8abd-4896-8856-8d02e2151fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save configuration strings to files\n",
    "make_dirs(job_dir)\n",
    "with open(f'{job_dir}/project.yml', 'w') as f:\n",
    "    f.write(yml_config_str.format(package_lang=lang, package_name=pkg_name_short,\n",
    "                                  package_version=version, gpu=gpu_id))\n",
    "\n",
    "make_dirs(f'{job_dir}/configs')\n",
    "with open(f'{job_dir}/configs/default.cfg', 'w') as f:\n",
    "    f.write(exp_config_str.format(vectors_dir=vectors_dir, allocator=gpu_allocator,\n",
    "                                  batch_size=batch_size, patience=patience))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d92ca-840e-490e-bf71-59ebd139b9f4",
   "metadata": {},
   "source": [
    "## Downloading data assets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dded8afe-4e01-4d18-95f2-37f462a4f631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m spacy project assets job_data\n"
     ]
    }
   ],
   "source": [
    "args = ['python', '-m', 'spacy', 'project', 'assets', job_dir]\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9efff419-c104-4fdb-8acb-778f64c16381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Fetching 1 asset(s)\u001B[0m\n",
      "\u001B[38;5;2m✔ Downloaded asset\n",
      "/Users/emiliano/Development/spacy_norwegian_training_test/job_data/assets/norne\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project assets job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7694350-daa4-4a0c-bb7e-d9c540f86a6c",
   "metadata": {},
   "source": [
    "## Running job tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa01c46b-feed-46ba-9ed3-53c24a088e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python -m spacy project run all job_data\n"
     ]
    }
   ],
   "source": [
    "args = ['python', '-m', 'spacy', 'project', 'run', 'all', job_dir]\n",
    "\n",
    "print(' '.join(args))  # copy output and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac65c0d2-82d5-41eb-b496-c244a29a9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Running workflow 'all'\u001B[0m\n",
      "\u001B[1m\n",
      "================================= preprocess =================================\u001B[0m\n",
      "\u001B[38;5;4mℹ Skipping 'preprocess': nothing changed\u001B[0m\n",
      "\u001B[1m\n",
      "=================================== train ===================================\u001B[0m\n",
      "Running command: /Users/emiliano/Development/spacy_norwegian_training_test/env/bin/python -m spacy train configs/default.cfg --output training/norne --gpu-id -1 --paths.train corpus/norne/train.spacy --paths.dev corpus/norne/dev.spacy --nlp.lang=nb\n",
      "\u001B[38;5;4mℹ Saving to output directory: training/norne\u001B[0m\n",
      "\u001B[38;5;4mℹ Using CPU\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4mℹ Pipeline: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler',\n",
      "'lemmatizer', 'ner']\u001B[0m\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.001\u001B[0m\n",
      "E    #       LOSS TOK2VEC  LOSS MORPH...  LOSS PARSER  LOSS NER  POS_ACC  MORPH_ACC  DEP_UAS  DEP_LAS  SENTS_F  LEMMA_ACC  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  -------------  -----------  --------  -------  ---------  -------  -------  -------  ---------  ------  ------  ------  ------\n",
      "/Users/emiliano/Development/spacy_norwegian_training_test/env/lib/python3.11/site-packages/spacy/pipeline/attributeruler.py:149: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  matches = self.matcher(doc, allow_missing=True, as_spans=False)\n",
      "  0       0          0.00         199.01       399.83     98.21    24.93      21.91    20.52     3.77     0.17      56.43    0.00    0.00    0.00    0.14\n",
      "  0    1000      18838.80       47600.99     97425.43   8608.74    95.05      91.08    80.66    74.50    88.62      75.66   63.09   63.87   62.32    0.75\n",
      "  1    2000      36372.89       37438.10    108889.18   7193.47    96.09      93.73    84.93    80.04    91.37      76.19   73.86   74.76   72.99    0.81\n",
      "  4    3000      98163.07       65439.88    233761.83  10005.88    96.66      94.99    86.81    82.98    92.89      76.37   76.27   77.15   75.41    0.83\n",
      "  8    4000     121634.42       52558.36    232161.58   5888.18    96.73      95.16    87.88    84.33    93.70      76.41   75.64   76.42   74.88    0.83\n",
      " 12    5000     126679.35       40858.74    201611.66   3940.49    96.73      95.35    88.25    84.69    94.23      76.46   75.34   76.31   74.41    0.83\n",
      " 16    6000     134530.88       34198.13    182901.13   2719.02    96.79      95.40    88.40    84.95    94.50      76.44   75.21   76.03   74.41    0.83\n",
      " 19    7000     141627.37       29894.04    167550.98   2104.58    96.78      95.42    88.43    85.05    94.46      76.48   74.51   75.46   73.58    0.83\n",
      "\u001B[38;5;2m✔ Saved pipeline to output directory\u001B[0m\n",
      "training/norne/model-last\n",
      "\u001B[1m\n",
      "================================== evaluate ==================================\u001B[0m\n",
      "Running command: /Users/emiliano/Development/spacy_norwegian_training_test/env/bin/python -m spacy evaluate ./training/norne/model-best ./corpus/norne/test.spacy --output ./metrics/metrics.json --gpu-id -1\n",
      "\u001B[38;5;4mℹ Using CPU\u001B[0m\n",
      "/Users/emiliano/Development/spacy_norwegian_training_test/env/lib/python3.11/site-packages/spacy/pipeline/attributeruler.py:149: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  matches = self.matcher(doc, allow_missing=True, as_spans=False)\n",
      "\u001B[1m\n",
      "================================== Results ==================================\u001B[0m\n",
      "\n",
      "TOK      99.76\n",
      "TAG      0.00 \n",
      "POS      96.50\n",
      "MORPH    94.70\n",
      "LEMMA    77.16\n",
      "UAS      87.97\n",
      "LAS      84.66\n",
      "NER P    72.77\n",
      "NER R    70.35\n",
      "NER F    71.54\n",
      "SENT P   95.07\n",
      "SENT R   94.48\n",
      "SENT F   94.77\n",
      "SPEED    17685\n",
      "\n",
      "\u001B[1m\n",
      "============================== MORPH (per feat) ==============================\u001B[0m\n",
      "\n",
      "                P        R        F\n",
      "Definite    97.22    96.48    96.85\n",
      "Gender      93.07    91.95    92.50\n",
      "Number      96.89    95.80    96.34\n",
      "Mood        98.27    98.10    98.19\n",
      "Tense       98.38    98.27    98.32\n",
      "VerbForm    97.51    96.89    97.20\n",
      "Degree      95.93    95.42    95.67\n",
      "Animacy     99.90    99.41    99.66\n",
      "Case        98.82    98.27    98.54\n",
      "Person      99.13    98.86    98.99\n",
      "PronType    98.89    98.64    98.76\n",
      "Polarity    99.68   100.00    99.84\n",
      "Poss        99.36    98.73    99.05\n",
      "Reflex     100.00   100.00   100.00\n",
      "NumType     99.71    83.82    91.08\n",
      "Voice       97.00    97.00    97.00\n",
      "Abbr        98.53    87.01    92.41\n",
      "\n",
      "\u001B[1m\n",
      "=============================== LAS (per type) ===============================\u001B[0m\n",
      "\n",
      "                    P       R       F\n",
      "nsubj           87.20   87.69   87.45\n",
      "cop             84.05   88.06   86.01\n",
      "amod            91.36   91.58   91.47\n",
      "root            90.08   89.48   89.78\n",
      "conj            71.54   71.42   71.48\n",
      "cc              91.77   91.68   91.72\n",
      "det             94.31   95.00   94.65\n",
      "obj             86.48   87.33   86.90\n",
      "acl:relcl       74.19   74.39   74.29\n",
      "case            93.25   93.06   93.15\n",
      "advmod          82.82   82.72   82.77\n",
      "compound:prt    73.73   75.98   74.84\n",
      "aux             92.05   93.33   92.69\n",
      "mark            91.70   91.58   91.64\n",
      "acl             66.45   56.28   60.95\n",
      "nmod            74.28   80.38   77.21\n",
      "advcl           70.25   69.53   69.89\n",
      "obl             76.49   71.51   73.92\n",
      "ccomp           83.11   83.48   83.30\n",
      "flat:name       84.58   79.64   82.04\n",
      "xcomp           70.43   68.95   69.68\n",
      "aux:pass        92.06   94.31   93.17\n",
      "nsubj:pass      84.36   80.32   82.29\n",
      "appos           38.46   20.55   26.79\n",
      "expl            70.35   75.86   73.00\n",
      "csubj           75.53   68.27   71.72\n",
      "iobj            68.75   54.10   60.55\n",
      "nummod          94.25   95.95   95.09\n",
      "parataxis       80.26   77.22   78.71\n",
      "compound        76.47   37.14   50.00\n",
      "acl:cleft       66.67   33.33   44.44\n",
      "dep              0.00    0.00    0.00\n",
      "orphan          33.33    3.85    6.90\n",
      "csubj:pass       0.00    0.00    0.00\n",
      "discourse      100.00   36.36   53.33\n",
      "flat:foreign    66.67    7.06   12.77\n",
      "goeswith         0.00    0.00    0.00\n",
      "\n",
      "\u001B[1m\n",
      "=============================== NER (per type) ===============================\u001B[0m\n",
      "\n",
      "              P       R       F\n",
      "PER       80.45   83.16   81.78\n",
      "GPE_LOC   79.70   84.05   81.82\n",
      "GPE_ORG   52.08   50.00   51.02\n",
      "PROD      28.21   15.49   20.00\n",
      "ORG       65.20   62.46   63.80\n",
      "DRV       85.00   70.83   77.27\n",
      "LOC       52.70   37.86   44.07\n",
      "EVT       11.11   20.00   14.29\n",
      "\n",
      "\u001B[38;5;2m✔ Saved results to metrics/metrics.json\u001B[0m\n",
      "\u001B[1m\n",
      "================================== package ==================================\u001B[0m\n",
      "Running command: /Users/emiliano/Development/spacy_norwegian_training_test/env/bin/python -m spacy package training/norne/model-best packages --name nhst_fasttext_300_10000 --version 2.0.0 --force\n",
      "\u001B[38;5;4mℹ Building package artifacts: sdist\u001B[0m\n",
      "\u001B[38;5;2m✔ Loaded meta.json from file\u001B[0m\n",
      "training/norne/model-best/meta.json\n",
      "\u001B[38;5;2m✔ Generated README.md from meta.json\u001B[0m\n",
      "\u001B[38;5;2m✔ Successfully created package directory\n",
      "'nb_nhst_fasttext_300_10000-2.0.0'\u001B[0m\n",
      "packages/nb_nhst_fasttext_300_10000-2.0.0\n",
      "running sdist\n",
      "running egg_info\n",
      "creating nb_nhst_fasttext_300_10000.egg-info\n",
      "writing nb_nhst_fasttext_300_10000.egg-info/PKG-INFO\n",
      "writing dependency_links to nb_nhst_fasttext_300_10000.egg-info/dependency_links.txt\n",
      "writing entry points to nb_nhst_fasttext_300_10000.egg-info/entry_points.txt\n",
      "writing requirements to nb_nhst_fasttext_300_10000.egg-info/requires.txt\n",
      "writing top-level names to nb_nhst_fasttext_300_10000.egg-info/top_level.txt\n",
      "writing manifest file 'nb_nhst_fasttext_300_10000.egg-info/SOURCES.txt'\n",
      "reading manifest file 'nb_nhst_fasttext_300_10000.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "warning: no files found matching 'LICENSE'\n",
      "warning: no files found matching 'LICENSES_SOURCES'\n",
      "writing manifest file 'nb_nhst_fasttext_300_10000.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/attribute_ruler\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/lemmatizer\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/lemmatizer/lookups\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/morphologizer\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/senter\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tok2vec\n",
      "creating nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "copying files to nb_nhst_fasttext_300_10000-2.0.0...\n",
      "copying MANIFEST.in -> nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying README.md -> nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying meta.json -> nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying setup.py -> nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying nb_nhst_fasttext_300_10000/__init__.py -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000\n",
      "copying nb_nhst_fasttext_300_10000/meta.json -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/PKG-INFO -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/SOURCES.txt -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/dependency_links.txt -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/entry_points.txt -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/not-zip-safe -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/requires.txt -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000.egg-info/top_level.txt -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000.egg-info\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/README.md -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/config.cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/meta.json -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tokenizer -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/attribute_ruler/patterns -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/attribute_ruler\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/lemmatizer/lookups/lookups.bin -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/lemmatizer/lookups\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/morphologizer/cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/morphologizer\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/morphologizer/model -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/morphologizer\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner/cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner/model -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner/moves -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/ner\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser/cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser/model -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser/moves -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/parser\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/senter/cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/senter\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/senter/model -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/senter\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tok2vec/cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tok2vec\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tok2vec/model -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/tok2vec\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab/key2row -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab/lookups.bin -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab/strings.json -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab/vectors -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "copying nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab/vectors.cfg -> nb_nhst_fasttext_300_10000-2.0.0/nb_nhst_fasttext_300_10000/nb_nhst_fasttext_300_10000-2.0.0/vocab\n",
      "Writing nb_nhst_fasttext_300_10000-2.0.0/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'nb_nhst_fasttext_300_10000-2.0.0' (and everything under it)\n",
      "\u001B[38;5;2m✔ Successfully created zipped Python package\u001B[0m\n",
      "packages/nb_nhst_fasttext_300_10000-2.0.0/dist/nb_nhst_fasttext_300_10000-2.0.0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy project run all job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3e69e-60fe-4361-bb64-4c9233e23e96",
   "metadata": {},
   "source": [
    "## Saving output\n",
    "\n",
    "- the necessary files are already saved, but we should move them to the final location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69805fbf-52ea-4c03-9701-4ce62daf9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_data/packages/nb_nhst_fasttext_300_10000-2.0.0/dist/nb_nhst_fasttext_300_10000-2.0.0.tar.gz\n",
      "job_data/metrics/metrics.json\n"
     ]
    }
   ],
   "source": [
    "# built automatically by Spacy, but we need it to copy the package\n",
    "pkg_file = f'{job_dir}/packages/{pkg_name_full}/dist/{pkg_name_full}.tar.gz'\n",
    "print(pkg_file)\n",
    "\n",
    "# evaluation metrics\n",
    "metrics_file = f'{job_dir}/metrics/metrics.json'\n",
    "print(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996c806b-f6b5-4374-8765-084987e2cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## token_acc ##\n",
      "0.9975565671\n",
      "\n",
      "## token_p ##\n",
      "0.9974896238\n",
      "\n",
      "## token_r ##\n",
      "0.9944937596\n",
      "\n",
      "## token_f ##\n",
      "0.9959894389\n",
      "\n",
      "## pos_acc ##\n",
      "0.9650153089\n",
      "\n",
      "## morph_acc ##\n",
      "0.9469792033\n",
      "\n",
      "## morph_micro_p ##\n",
      "0.9690645618\n",
      "\n",
      "## morph_micro_r ##\n",
      "0.9606014503\n",
      "\n",
      "## morph_micro_f ##\n",
      "0.9648144473\n",
      "\n",
      "## morph_per_feat ##\n",
      "{'Definite': {'p': 0.9721749006, 'r': 0.9647788109, 'f': 0.9684627351}, 'Gender': {'p': 0.9307011414, 'r': 0.9194569094, 'f': 0.9250448573}, 'Number': {'p': 0.9689208895, 'r': 0.9580066685, 'f': 0.9634328696}, 'Mood': {'p': 0.9827288428, 'r': 0.9810344828, 'f': 0.9818809318}, 'Tense': {'p': 0.9838009835, 'r': 0.98266397, 'f': 0.983232148}, 'VerbForm': {'p': 0.9750869061, 'r': 0.9689119171, 'f': 0.9719896044}, 'Degree': {'p': 0.9592559787, 'r': 0.954185022, 'f': 0.9567137809}, 'Animacy': {'p': 0.9990176817, 'r': 0.9941348974, 'f': 0.9965703087}, 'Case': {'p': 0.988178025, 'r': 0.9827109267, 'f': 0.9854368932}, 'Person': {'p': 0.9912758997, 'r': 0.9885807504, 'f': 0.9899264906}, 'PronType': {'p': 0.9889473684, 'r': 0.986351706, 'f': 0.9876478318}, 'Polarity': {'p': 0.9968454259, 'r': 1.0, 'f': 0.9984202212}, 'Poss': {'p': 0.9936305732, 'r': 0.9873417722, 'f': 0.9904761905}, 'Reflex': {'p': 1.0, 'r': 1.0, 'f': 1.0}, 'NumType': {'p': 0.9971264368, 'r': 0.8381642512, 'f': 0.9107611549}, 'Voice': {'p': 0.97, 'r': 0.97, 'f': 0.97}, 'Abbr': {'p': 0.9852941176, 'r': 0.8701298701, 'f': 0.924137931}}\n",
      "\n",
      "## sents_p ##\n",
      "0.9507005708\n",
      "\n",
      "## sents_r ##\n",
      "0.9448169159\n",
      "\n",
      "## sents_f ##\n",
      "0.947749612\n",
      "\n",
      "## dep_uas ##\n",
      "0.8797185869\n",
      "\n",
      "## dep_las ##\n",
      "0.8466222861\n",
      "\n",
      "## dep_las_per_type ##\n",
      "{'nsubj': {'p': 0.872027972, 'r': 0.8769338959, 'f': 0.8744740533}, 'cop': {'p': 0.8404558405, 'r': 0.8805970149, 'f': 0.860058309}, 'amod': {'p': 0.9135702746, 'r': 0.9157894737, 'f': 0.9146785281}, 'root': {'p': 0.9008307373, 'r': 0.8947911294, 'f': 0.8978007762}, 'conj': {'p': 0.7154185022, 'r': 0.7141600704, 'f': 0.7147887324}, 'cc': {'p': 0.9176706827, 'r': 0.9167502508, 'f': 0.9172102358}, 'det': {'p': 0.9431089744, 'r': 0.9499596449, 'f': 0.946521914}, 'obj': {'p': 0.8648083624, 'r': 0.8733286418, 'f': 0.869047619}, 'acl:relcl': {'p': 0.7419354839, 'r': 0.74393531, 'f': 0.7429340511}, 'case': {'p': 0.9325396825, 'r': 0.930551325, 'f': 0.9315444428}, 'advmod': {'p': 0.8281596452, 'r': 0.8272425249, 'f': 0.827700831}, 'compound:prt': {'p': 0.7372881356, 'r': 0.7598253275, 'f': 0.7483870968}, 'aux': {'p': 0.920466596, 'r': 0.9333333333, 'f': 0.9268553123}, 'mark': {'p': 0.9170096022, 'r': 0.9157534247, 'f': 0.9163810829}, 'acl': {'p': 0.664516129, 'r': 0.5628415301, 'f': 0.6094674556}, 'nmod': {'p': 0.742768595, 'r': 0.8038010061, 'f': 0.7720805369}, 'advcl': {'p': 0.7024793388, 'r': 0.6952965235, 'f': 0.6988694758}, 'obl': {'p': 0.7648709315, 'r': 0.7151101784, 'f': 0.739154013}, 'ccomp': {'p': 0.8311111111, 'r': 0.8348214286, 'f': 0.8329621381}, 'flat:name': {'p': 0.8458244111, 'r': 0.7963709677, 'f': 0.8203530633}, 'xcomp': {'p': 0.7043010753, 'r': 0.6894736842, 'f': 0.6968085106}, 'aux:pass': {'p': 0.9206349206, 'r': 0.9430894309, 'f': 0.9317269076}, 'nsubj:pass': {'p': 0.843575419, 'r': 0.8031914894, 'f': 0.8228882834}, 'appos': {'p': 0.3846153846, 'r': 0.2054794521, 'f': 0.2678571429}, 'expl': {'p': 0.7034883721, 'r': 0.7586206897, 'f': 0.730015083}, 'csubj': {'p': 0.7553191489, 'r': 0.6826923077, 'f': 0.7171717172}, 'iobj': {'p': 0.6875, 'r': 0.5409836066, 'f': 0.6055045872}, 'nummod': {'p': 0.9424778761, 'r': 0.9594594595, 'f': 0.9508928571}, 'parataxis': {'p': 0.8026315789, 'r': 0.7721518987, 'f': 0.7870967742}, 'compound': {'p': 0.7647058824, 'r': 0.3714285714, 'f': 0.5}, 'acl:cleft': {'p': 0.6666666667, 'r': 0.3333333333, 'f': 0.4444444444}, 'dep': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'orphan': {'p': 0.3333333333, 'r': 0.0384615385, 'f': 0.0689655172}, 'csubj:pass': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'discourse': {'p': 1.0, 'r': 0.3636363636, 'f': 0.5333333333}, 'flat:foreign': {'p': 0.6666666667, 'r': 0.0705882353, 'f': 0.1276595745}, 'goeswith': {'p': 0.0, 'r': 0.0, 'f': 0.0}}\n",
      "\n",
      "## tag_acc ##\n",
      "0.0\n",
      "\n",
      "## lemma_acc ##\n",
      "0.7716374709\n",
      "\n",
      "## ents_p ##\n",
      "0.7277486911\n",
      "\n",
      "## ents_r ##\n",
      "0.7035430224\n",
      "\n",
      "## ents_f ##\n",
      "0.7154411765\n",
      "\n",
      "## ents_per_type ##\n",
      "{'PER': {'p': 0.8044596913, 'r': 0.8315602837, 'f': 0.8177855275}, 'GPE_LOC': {'p': 0.7970479705, 'r': 0.8404669261, 'f': 0.8181818182}, 'GPE_ORG': {'p': 0.5208333333, 'r': 0.5, 'f': 0.5102040816}, 'PROD': {'p': 0.2820512821, 'r': 0.1549295775, 'f': 0.2}, 'ORG': {'p': 0.652014652, 'r': 0.6245614035, 'f': 0.6379928315}, 'DRV': {'p': 0.85, 'r': 0.7083333333, 'f': 0.7727272727}, 'LOC': {'p': 0.527027027, 'r': 0.3786407767, 'f': 0.4406779661}, 'EVT': {'p': 0.1111111111, 'r': 0.2, 'f': 0.1428571429}}\n",
      "\n",
      "## speed ##\n",
      "17685.040782195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check metrics\n",
    "with open(f'{job_dir}/metrics/metrics.json') as f:\n",
    "    metrics = json.load(f)\n",
    "    for k, v in metrics.items():\n",
    "        print('##', k, '##')\n",
    "        print(v)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf6092-d89a-4857-b508-44a214917365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfedc12-1956-4c01-88fa-19a08c94c487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
